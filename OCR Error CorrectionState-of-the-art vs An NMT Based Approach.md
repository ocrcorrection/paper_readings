# OCR Error Correction:State-of-the-art vs An NMT Based Approach



## 摘要

尽管最新的OCR系统的性能非常之好，它们依然会因为各种原因产生错误。并且，当它们被应用在一些有着陈旧原稿的历史文档时，情况会变得更糟。这就是为什么OCR后处理错误矫正会成为多年来的一个热门问题。最近几年，有很多最新进展的OCR方法被提出。本文通过介绍两种新颖的促进OCR系统准确度的深度学习方法，和一个能够进一步加强输出结果质量的后处理技巧来对OCR后处理错误纠正领域有所贡献。这些方法基于神经机器翻译（NMT）并且受激励于深度学习被引入NLP领域后的重大成功。最后，我们会用OCR后处理错误纠正的最新方法同本文介绍的新系统进行比较，并讨论比较结果。



## 介绍

自从第一台机器被发明直至今天，人类活动的主要目的就是复制人类自身的能力并生产能够相对人类更加高效高精确地完成任务的机器。其中一项能力就是阅读。最近五十年，机器阅读已经发生了重大改变，并且对于这样的技术的需要也开始与日俱增。今天OCR系统已经能够转换打字，手写，打印等各种文本图片为机器编码的可检索的文本，无论是来自被扫描文件，文档照片，场景照片还是图像上叠加的字幕文字。尽管众多OCR系统已经取得了巨大成功，它们仍然不能被应用于人类生活，因为它们会在阅读文档时产错误。这些错误主要源于例如低图片质量或者一页上的复杂结构。而且，这些错误在处理历史文件时很明显变得更多，因为这些文件可能包含一些具有挑战性的字母和质量下降，如包括连字，历史拼写辩题，位置偏移的历史字体，由于墨水随着时间的推移逐渐渗入纸张的边界模糊，纸张降解导致的深色背景，斑点，裂缝，污垢，并从下一页往上的渗透。扫描书籍等大型文档时，OCR错误率令人满意，例如，可以有助于降低手动转录这些文档的时间和成本。而且，这种OCR系统在诸如历史文档的数字化之类的应用中可以可靠地保存其内容。因此，需要尽可能地减少OCR输出中的错误。

近年来，为了纠正人类产生的打字错误，进行了许多研究，无论是语法错误还是拼写错误。但是，这些错误本质上与OCR系统产生的错误不同，因为人为错误不像OCR错误那样可预测。在大多数情况下，可以使用样本输出预测OCR错误，因为OCR系统在每次读取相同字符时往往会犯同样的错误。

在本文中，首先，我们将介绍两种用于Post-OCR校正的新型深度学习架构。此外，我们将介绍一种后处理技术，以帮助进一步提高两种模型的准确性。最后，将这些新方法与现有技术进行了比较，并讨论了结果。选择深度神经网络来解决OCR后纠错问题背后的动机主要是他们在过去十年中在不同领域取得的成功，以及最近几年在自然语言处理领域取得的令人难以置信的成果。第二个原因是大多数关于OCR校正的研究论文都使用了最先进的技术，主要是统计语言建模。本文的其余部分安排如下。 第二节简要介绍了神经机器翻译。OCR校正方法在第III节中介绍。 第四节介绍了实验和结果，然后是第五节的结论。



## NEURAL MACHINE TRANSLATION(NMT)

神经机器翻译（NMT）是一种使用神经网络的机器翻译方法。几十年来，机器翻译主要通过划分来完成将给定的句子分成多个部分，然后尝试为每个部分提供翻译。这些方法导致翻译缺乏语言的流畅性，并且由于语法规则不同，大多数时候翻译在目标语言中是乱序的。统计机器翻译的最新发展，通过找到重排输出句子的以遵守语法规则的新方法，进行更自然的翻译。然而，所有这些方法都远非人类的自然行为，即阅读整个源句，理解意义，然后仅基于意义构建目标句子，忽略单词的一对一翻译。这才是NMT的工作原理。

NMT在以下流程中工作。 首先，源句子被送到网络的编码器，该编码器输出一组数值。 这些值表示独立于字典的句子的含义。 之后，这些值被馈送到网络的解码器中，该解码器可以构造具有相同含义的新句子，但是，具有目标语言的词汇和语法。这个流程被称为编码-解码架构，见图1。(额，就不引用占地方了，就是普通的编码解码结构)

这些编码器和解码器可以使用不同的神经网络架构来实现。 但是，因为我们正在处理顺序数据（句子），所以对于编码器和解码器来说，使用递归神经网络（RNN）是很自然的。RNN具有许多架构风格， 就使用长短期记忆（LSTM）或门控递归单位（GRU）的RNN小区，网络深度等而言。

在所提出的模型中使用的RNN小区是LSTM。这种选择的动机是LSTM RNN在语音识别，语言建模，翻译和图像字幕领域的出色表现。

在本文中，将使用OpenNMT构建神经网络架构。 OpenNMT是NMT的开源工具包，使用户能够自定义RNN以适应其所用的应用程序。



## METHODOLOGY(OCR校正方法)

在本节中，介绍了两种不同的RNN架构，以提高OCR输出的准确性。这两种架构几乎相同，但网络的输入和输出格式略有变化。之后，我们将介绍一种后处理方法，可以帮助提高引入模型的准确性。

###A. Word Based Sequence-to-Sequence model(基于词的端到端模型)

模型基于编码-译码RNN架构。这个想法是将Post-OCR校正问题视为翻译问题。这是通过将源语言视为错误的OCR输出，并将校正的输出视为目标语言来完成的。该模型在单词级别上工作，因为它将句子编码为单词序列。输入字典是OCR输出中使用的字集。而输出字典是在预处理步骤期间从目标语言中提取的单词集。

在此模型中，我们将使用具有以下规格的RNN（这个架构是根据我们手头的数据集进行实验测试而选择的）

i)深度（编码器/解码器内的LSTM单元层数）：3

ii)LSTM隐藏状态的数量：1024

iii)单词嵌入规模：1024

iv)dropout比率：30% 

v)optimizer:随机梯度下降

vi)learning rate: 1

vii)learning rate decay rate:50%

viii)start learning decay at epoch:8

ix)number of epochs:20

![捕获](C:\Users\xiongkaiqi\Pictures\开发使用\123\捕获.PNG)



该模型的输入是OCR输出和相应的基础事实的平行语料库。在将输入馈送到模型进行训练之前，首先我们必须按如下方式对其进行预处理：

*1.每个句子都应该被token化，以便每个单词与任何标点符号分开，见图3。*（差不多就是把贴在一起的标点给拆出来以免作为一个token）

![figure](C:\Users\xiongkaiqi\Pictures\开发使用\123\figure.PNG)

*2.构建源语言和目标语言的字典（每个语料库中的单词集）。*



通常，基于Word的模型具有以下优点：

*1.它学习定义正确句子的单词之间的**关系**（它产生语法正确的句子）。*

*2.它学习常见的单词错误及其更正。*

3.校正后的序列不必与错误序列长度相同（它可以处理单词完全丢失的情况）。



基于Word的模型的缺点：

1.如果一个单词不在输出字典中，模型就不会纠正它（可以纠正一组有限的单词，但仅限训练集单词）。

2.模型无法学习到字母级别的错误（它只能纠正一个单词的整体）

3.它需要相对大量的并行数据进行训练（它需要建立一个基于word的大型词汇表来减少看不见的词语问题的影响，参考第一个缺点）。



### B.基于字符的端到端模型

这种方法与基于单词的模型几乎相同。但是，体系结构和输入/输出格式略有变化。在基于字符的模型中，**字典是基于目标语言的字符构建的**。这样，我们可以在**相对较小的数据集**上训练校正模型，因为神经网络是在学习字符级别的校正。但是，训练数据越多，结果越好，因为它学习了OCR系统的行为以及如何纠正错误。

在此模型中，我们将使用具有以下规范的RNN（根据我们对手中数据集的实验测试选择此架构）：

i)深度（编码器/解码器内的LSTM单元层数）：4

ii)LSTM隐藏状态的数量：1024

iii)单词嵌入规模：1024

iv)dropout比率：30% 

v)optimizer:ADAM

vi)learning rate:0.001

vii)learning rate decay rate:50%

viii)start learning decay at epoch:11

ix)number of epochs:20

![figure4](C:\Users\xiongkaiqi\Pictures\开发使用\123\figure4.PNG)

该模型的输入是OCR输出和相应基础事实的平行语料库。 在将输入馈送到模型进行训练之前，首先我们必须按如下方式对其进行预处理：

![figure5](C:\Users\xiongkaiqi\Pictures\开发使用\123\figure5.PNG)

1.每个句子都应该被标记化，以便每个字符用空格与任何其他字符分隔，空格字符用标记<SPACE>替换，见图5。

2.构建源语言和目标语言的字典（每个语料库中的字符集）。



一般来说，这种基于字符的模型不会受到基于Word的模型的缺点，事实上，它解决了大多数基于Word的模型问题，因为它具有以下优点：

1.它学习常见的字符错误及其更正（它可以纠正单个字符而不是整个单词）。

2.它可以通过纠正单个字符来纠正以前从未见过的单词。

3.校正后的序列不必与错误序列长度相同（它可以处理丢失多余的字符）。

4.与基于单词的方法相比，它需要较少的训练数据，以开始提供建议。

然而，这种基于角色的方法有一个缺点，它有时产生不在语言中的单词。 只要它给出一次对字符错误的纠正建议，就会发生这种情况。



### c.单词级别预测的标准化

虽然之前的模型本身显示出良好的输出结果，但是参见图6，基于单词的模型在处理看不见的单词时遇到问题，不是忽略它们，而是试图根据之前学到的内容给出建议，从而导致输出与OCR输入完全不同的字，见图7。基于这个原因，引入了规范化的概念。

![figure6](C:\Users\xiongkaiqi\Pictures\开发使用\123\figure6.PNG)

![figure7](C:\Users\xiongkaiqi\Pictures\开发使用\123\figure7.PNG)

规范化方法是一种应用于OCR错误输出和相应校正建议的技术，它应用以下步骤：

1.使用*Levenshtein编辑距离*将OCR输出与神经网络建议对齐，以便OCR输出中的每个单词都使用其中的相应单词进行处理

（Levenshtein编辑距离：由一个序列经历增删查到达另一个序列的最少步骤数）

2.循环遍历单词，并根据两个单词之间的编辑距离选择OCR输出或建议的校正，同时我们根据单词长度设置动态阈值

3.输出结果句子

规范化可以解决图7中出现的问题，并给出图8中的输出：

![figure8](C:\Users\xiongkaiqi\Pictures\开发使用\123\figure8.PNG)



![algorithm1](C:\Users\xiongkaiqi\Pictures\开发使用\123\algorithm1.PNG)

伪码解释：先对输入和预测结果进行单词的1->1(一一看起来太像破折了)映射（align），即将输入的词映射到预测值中（此处猜测预测值可能进行过重新排序，需要保存重排记录才能完成映射）,遍历每一个词，根据词的长度设置匹配阈值1,3,5,计算出输入与预测结果的编辑距离，预测结果长度减去编辑距离得到一个近似的匹配值（应该略小于实际相似度，所以阈值也偏小），匹配值大于阈值选择时选择预测值作为输出，匹配值过小则选择原输入



## IV 实验设计

在本节中，我们将介绍用于培训和测试所有方法的数据集。 同样对于每个数据集，我们给出简要描述并讨论如何收集它。之后，我们将解释用于评估系统的评估指标

### A.数据集

我们尝试了不同语言的不同数据集。 实际上，我们使用三个数据集，一个数据集用于以下每种语言的英语，德语和中世纪拉丁语（15世纪）。根据表I中的百分比将数据集分成训练，验证和测试集。以下是有关所用数据集的一些详细信息：

1）英语数据集：这是当代英语的数据集。 它由两个平行部分组成，OCR输出和相应的基础事实。数据主要来自以下两个来源：

UNLV数据集：该数据集包含2889页来自各种来源的扫描文档图像（杂志，报纸，商业信函，年度报告等）。 扫描图像以200和300 DPI分辨率以双色，灰色和传真格式提供。 在原始数据集旁边提供了基础事实数据，其中包含手动标记的区域; 区域类型以文本格式提供。

UW-III数据集：数据集由1600个斜交校正的英文文档图像组成，其中包含手动编辑的实体边界框的基础事实。 这些边界框包含页面框架，文本和非文本区域，文本行和单词。 每个区域的类型（文本，数学，表格，图形等）也会被标记。

通过**逐行裁剪每条线**，逐行从两个数据集中提取数据，然后在其上运行anyOCR [6]系统。 这种裁剪技术背后的动机是通过强制阅读顺序和避免线分割来消除尽可能多的错误。 最后，将数据合并为两个并行文档（OCR输出和基础事实），每行包含一个句子。【anyOCR:对于未标注历史文档的基于序列学习的OCR系统】

### B.评价矩阵（就说明了一下用的什么参数进行度量）

评估the suggested corrections改善或降低OCR输出准确性的效果的过程，是将新引入的方法与最新方法进行比较的关键要素之一。 在OCR校正领域的大多数研究论文中，最常用的两种测量方法是在OCR输出上**应用校正模型之前和之后的字符错误率（CER）和字错误率（WER）**。

错误率是一种度量，它根据将文本转换为其基本事实所需的Levenshtein编辑操作（插入，删除和替换）的总数来衡量给定文本与其基本事实之间的差异。



## V 表现评估

在本节中，我们将介绍所进行的实验及其结果。 我们用六种不同的方法进行了实验，以便将新引入的方法的结果与最先进的技术进行比较。 使用的方法如下：

最新方法：

- Uni-gram language modeling(Uni-gram)

- Statistical Machine Translation(SMT)

本文引入的新方法：

- Word based model (WBM)
- Character based model (CBM)
- Word based model with Normalization (WBM++)
- Character based model with Normalization(CBM++) 

以下是我们在分析实验结果后变得明显的一些重要观察结果：

一般来说，纠正字符的模型比纠正整个单词的模型表现更好，这主要是由于所有基于单词的模型共享的问题，即它们不能纠正新看到的单词。

基于单词的模型倾向于获得相当高的CER分数，因为它们通常取代任何新出现的单词，而不是在训练阶段之前已经看过的单词

**标准化技术**显示了**基于单词的模型的结果的显著改进**，因为它直接针对新出现的单词问题。 另一方面，**标准化并没有显著改善基于字符的模型的结果**，因为这样的模型不会受到新看到的单词问题的影响。

（这里其实没搞明白基于字符是咋按它的算法标准化的，如果按照基于单词同样的标准化方法感觉毫无必要，理由就是上面这个结论）

（笑抽，google翻译把显著输出成了显着，这应该是后面字典的错误）

尽管对基于单词的模型引入了规范化的改进。 表V中的拉丁数据集的结果仍然不好，因为拉丁数据集遭受另一个问题，即大小，因为它明显小于所有其他数据集。

在英语的情况下，结果没有显着改善，因为OCR输出本身非常准确，在字符级别上具有97.13％的准确度，在字级别上具有91.13％的准确度。（甚至在纠正后错误率无一例外都出现了不同程度的提升，原图其实就CBM和CBM++准确率提升了四项，只有四项原因就是英语准确率本来就有90%多，不好提上去，其它的都只有德语单词级别一项提升了准确率，注：一共6项）

性能最佳的模型是具有归一化（CBM ++）的基于字符的模型，因为它不需要大的训练数据来提供良好的性能，并且它还可以通过校正单个字符来校正新看到的单词。

## VI 总结

在本文中，我们尝试了OCR纠错中最先进的方法。 之后，基于LSTM回归神经网络在自然语言处理领域的成功，我们引入了两个新的深度神经网络，可以解决同样的问题。

。。。（一堆前面说过的）

结果表明，我们采用归一化技术（CBM ++）引入的基于字符的模型可以被采用并应用于实际应用中，以提高OCR系统的准确性，因为它优于最先进的方法。 即使训练数据被认为相对较小。

（强调了一下CBM++这个性能最好的，但是训练数据可能相对较小）




